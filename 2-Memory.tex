\subsection{Memory Management and File Access}
\label{sec:fileIO}
One of the primary goals of the \code{gadfly} project is to enable the analysis of large datasets on machines with limited memory.
Enabling this requires intelligent memory management, loading only the particle data of interest from the disk.
Fortunately the HDF5 protocol is well-suited to such non-contiguous file access, allowing not only individual data fields to be accessed independently, but also for the loading of select entries only from the field in question.

\code{Gadfly} allows two complementary approaches to minimizing the memory footprint.
The first method requires definition of a refinement criterion, such as particles above a given density threshold.
The resulting `refined' index can then be used to select only the corresponding values from subsequently loaded particle fields.
While this method is efficient, it is poorly suited to exploratory analysis where the proper refinement criterion may not be know {\it{a priori}}.  As such, this approach is best suited for use in scripts where the analysis to be performed is defined beforehand.

While use of a refinement criterion minimizes unnecessary I/O operations, this approach is fairly rigid, and on its own would fail when attempting to load additional fields into an existing \code{PartType} dataframe from which particles have been manually dropped by the user.
To mitigate this we take advantage of 
However, when additional fields are loaded into an existing  existing data are dropped.
This allows for the incremental refinement along several axes of the data kept in memory.
Additional cuts can be made as subsequent fields are loaded, resulting in the selection of a precisely targeted primary dataset from which derived properties (e.g., temperature) may be calculated, serving to reduce computational overhead as well.